import string
import re
import nltk
import emoji
from nltk import sent_tokenize, word_tokenize
from nltk.corpus import stopwords


def normalize(text):
    # Remove emojis
    text = emoji.get_emoji_regexp().sub(u'', text)
    # Tokenize the text
    tokens = word_tokenize(text)
    # Remove the punctuations
    tokens = [word.translate(str.maketrans(
        '', '', string.punctuation)) for word in tokens]
    # Lower the tokens
    tokens = [word.lower() for word in tokens]
    # Remove stopword
    tokens = [
        word for word in tokens if not word in stopwords.words("turkish")]
    # Remove multiple spaces in the sentence
    value = re.sub(' +', ' ', ' '.join(tokens))
    # Remove any leading and trailing whitespaces
    value = str.strip(value)
    return value


if __name__ == "__main__":
    sentences = [
        "Ä°stediÄŸim renkte gelmedi. Kalitesi de resimdeki gibi iyi deÄŸil. Bu fiyata bu Ã§anta fazla..",
        "Parmak arasÄ± sandaletlerin en rahatÄ±, en ÅŸÄ±kÄ±ğŸ™     ayakta yok hissi. DiÄŸer renklerini de alacaÄŸÄ±m. Åiddetle tavsiye edilir. TeÅŸekkÃ¼rler Oblavion ve Trendyol.",
        "muhteÅŸem Ã¶tesi. yumuÅŸacÄ±k rahat ve Ã§ok ÅŸÄ±k bir sandalet. bir numara bÃ¼yÃ¼k tercih edilmeliğŸ˜Š",
        "tam alacakken indirim kalktÄ± kÄ±sa zamanda yine indirim bekliyorum ğŸ¤—",
        "    abc    ",
        "AynÄ± Ã¶zellikte neredeyse iki katÄ± paraya isim yapmÄ±ÅŸ marka almaya lÃ¼zum yok. Kaliteli ve ucuz. Smart tv Ã¶zelliÄŸi yok ama Xioami mi tv box aldÄ±m, smart tvlerden on kat daha iyi. Kesinlikle tavsiye ederim."
    ]
    for text in sentences:
        print(normalize(text))
